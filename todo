## Phase 1 — Foundation (Bare Metal Raft)
- [x] **1.1** Project structure + Go modules setup
- [x] **1.2** Define core types: `NodeID`, `Term`, `LogEntry`, `State` (Follower/Candidate/Leader)
- [x] **1.3** Persistent state: `currentTerm`, `votedFor`, `log[]`
- [x] **1.4** In-memory Raft node struct with timers
- [x] **1.5** Election timeout + randomized timer logic
- [x] **1.6** `RequestVote` RPC (gRPC proto + handler)
- [x] **1.7** Leader election loop — become candidate, collect votes, become leader
- [x] **1.8** Heartbeat loop — leader sends empty `AppendEntries` to prevent re-election
- [x] **1.9** `AppendEntries` RPC — log replication + heartbeat handler
- [x] **1.10** Log matching property + conflict resolution

## Phase 2 — Log Replication & Commit
- [x] **2.1** Leader appends entries to log, tracks `nextIndex[]` / `matchIndex[]` per peer
- [x] **2.2** Commit index advancement (majority quorum check)
- [x] **2.3** Apply committed entries to state machine
- [x] **2.4** Follower log consistency repair (backtracking `nextIndex`)
- [x] **2.5** Write a simple key-value state machine (the "database")

## Phase 3 — Cluster Wiring (gRPC Transport)
- [x] **3.1** gRPC server per node
- [x] **3.2** Peer client pool (dial + reconnect logic)
- [x] **3.3** Multi-node startup (3 nodes, local or Docker)
- [ ] **3.4** Network partition simulation (drop packets between nodes)

## Phase 4 — Persistence & Crash Recovery
- [x] **4.1** WAL (Write-Ahead Log) — persist log entries to disk
- [x] **4.2** Persist `currentTerm` and `votedFor` (survive crashes)
- [x] **4.3** Restart a node and rejoin cluster — replay log
- [ ] **4.4** Snapshot mechanism (compact old log entries)
- [ ] **4.5** `InstallSnapshot` RPC (send snapshot to lagging follower)

## Phase 5 — Client Layer
- [ ] **5.1** Client library — connects to any node, auto-redirects to leader
- [ ] **5.2** Linearizable reads (read index or lease-based)
- [ ] **5.3** Retry logic + idempotency keys (exactly-once semantics)
- [x] **5.4** CLI client (`get`, `put`, `delete`)

## Phase 6 — Transactions (MVCC)
- [ ] **6.1** Versioned key-value store (MVCC storage layer)
- [ ] **6.2** Transaction begin / read / write / commit
- [ ] **6.3** Optimistic concurrency control (conflict detection at commit)
- [ ] **6.4** Two-phase commit coordinator across nodes

## Phase 7 — Observability Dashboard
- [ ] **7.1** Prometheus metrics (leader, term, log lag, commit index)
- [ ] **7.2** `/status` HTTP endpoint per node
- [ ] **7.3** Terminal dashboard (via `tview` or `bubbletea`) showing cluster state in real time

## Phase 8 — Hardening & Benchmarks
- [ ] **8.1** Chaos testing — kill nodes randomly, verify no data loss
- [ ] **8.2** Benchmark writes/sec with 3-node replication
- [ ] **8.3** Measure leader election latency (<2 seconds target)
- [ ] **8.4** Jepsen-style correctness verification (manual or automated)
